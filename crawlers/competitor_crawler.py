#!/usr/bin/env python3
"""
ç«å“çˆ¬è™« - æ”¯æŒKickstarterã€Redditç­‰å¹³å°
ä¸“æ³¨äº24å°æ—¶å†…çš„å†…å®¹çˆ¬å–
"""

import re
import time
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from loguru import logger

class CompetitorPost:
    """ç«å“å¸–å­æ•°æ®ç»“æ„"""
    def __init__(self):
        self.title: str = ""
        self.content: str = ""
        self.author: str = ""
        self.post_url: str = ""
        self.post_time: datetime = None
        self.likes_count: int = 0
        self.comments_count: int = 0
        self.platform: str = ""
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            "title": self.title,
            "content": self.content,
            "author": self.author,
            "post_url": self.post_url,
            "post_time": self.post_time.isoformat() if self.post_time else None,
            "likes_count": self.likes_count,
            "comments_count": self.comments_count,
            "platform": self.platform
        }

class CompetitorCrawler:
    """ç«å“çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        })
        self.driver = None
        
        # 24å°æ—¶æ—¶é—´èŒƒå›´
        self.time_cutoff = datetime.now() - timedelta(hours=24)
    
    def crawl_by_config(self, config: Dict[str, Any]) -> List[CompetitorPost]:
        """æ ¹æ®é…ç½®çˆ¬å–å†…å®¹"""
        posts = []
        
        if config['config_type'] == 'account':
            # è´¦å·é“¾æ¥æ¨¡å¼
            posts = self.crawl_account_posts(config['account_url'])
        elif config['config_type'] == 'keyword':
            # å…³é”®è¯æ¨¡å¼
            keywords = config['keywords'].split('/') if config['keywords'] else []
            posts = self.crawl_website_posts(config['website_url'], keywords)
        elif config['config_type'] == 'webpage_update':
            # ç½‘é¡µæ›´æ–°æ¨¡å¼
            posts = self.crawl_webpage_updates(config)
        
        # è¿‡æ»¤24å°æ—¶å†…çš„å†…å®¹
        recent_posts = []
        for post in posts:
            if post.post_time and post.post_time >= self.time_cutoff:
                recent_posts.append(post)
        
        logger.info(f"çˆ¬å–å®Œæˆï¼šæ€»è®¡ {len(posts)} æ¡ï¼Œ24å°æ—¶å†… {len(recent_posts)} æ¡")
        return recent_posts
    
    def crawl_account_posts(self, account_url: str) -> List[CompetitorPost]:
        """çˆ¬å–æŒ‡å®šè´¦å·çš„å¸–å­"""
        domain = urlparse(account_url).netloc.lower()
        
        if 'kickstarter.com' in domain:
            return self._crawl_kickstarter_account(account_url)
        elif 'reddit.com' in domain:
            return self._crawl_reddit_user(account_url)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„è´¦å·å¹³å°: {domain}")
            return []
    
    def crawl_website_posts(self, website_url: str, keywords: List[str]) -> List[CompetitorPost]:
        """çˆ¬å–ç½‘ç«™çš„å…³é”®è¯ç›¸å…³å¸–å­"""
        domain = urlparse(website_url).netloc.lower()
        
        if 'reddit.com' in domain:
            return self._crawl_reddit_subreddit(website_url, keywords)
        elif 'kickstarter.com' in domain:
            return self._crawl_kickstarter_search(website_url, keywords)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„ç½‘ç«™å¹³å°: {domain}")
            return []
    
    def _crawl_kickstarter_account(self, account_url: str) -> List[CompetitorPost]:
        """çˆ¬å–Kickstarterè´¦å·é¡µé¢"""
        posts = []
        
        try:
            logger.info(f"çˆ¬å–Kickstarterè´¦å·: {account_url}")
            
            # ä½¿ç”¨Seleniumå¤„ç†åŠ¨æ€å†…å®¹
            driver = self._init_webdriver()
            if not driver:
                return posts
            
            driver.get(account_url)
            time.sleep(3)
            
            # æŸ¥æ‰¾é¡¹ç›®åˆ—è¡¨
            project_elements = driver.find_elements(By.CSS_SELECTOR, '.project-card, .js-react-on-rails-component')
            
            for element in project_elements:
                try:
                    post = CompetitorPost()
                    post.platform = "Kickstarter"
                    
                    # æå–é¡¹ç›®æ ‡é¢˜
                    title_element = element.find_element(By.CSS_SELECTOR, 'h3 a, .project-title a')
                    if title_element:
                        post.title = title_element.text.strip()
                        post.post_url = title_element.get_attribute('href')
                    
                    # æå–ä½œè€…
                    author_element = element.find_element(By.CSS_SELECTOR, '.by-author a, .creator-name')
                    if author_element:
                        post.author = author_element.text.strip()
                    
                    # æå–æè¿°
                    desc_element = element.find_element(By.CSS_SELECTOR, '.project-description, .description')
                    if desc_element:
                        post.content = desc_element.text.strip()
                    
                    # æå–æ—¶é—´ï¼ˆKickstarteræ¯”è¾ƒå¤æ‚ï¼Œè®¾ç½®ä¸ºå½“å‰æ—¶é—´ï¼‰
                    post.post_time = datetime.now()
                    
                    if post.title:
                        posts.append(post)
                        
                except Exception as e:
                    logger.debug(f"è§£æKickstarteré¡¹ç›®å¤±è´¥: {e}")
                    continue
            
            driver.quit()
            
        except Exception as e:
            logger.error(f"çˆ¬å–Kickstarterè´¦å·å¤±è´¥: {e}")
            if self.driver:
                self.driver.quit()
        
        return posts
    
    def _crawl_reddit_subreddit(self, subreddit_url: str, keywords: List[str]) -> List[CompetitorPost]:
        """çˆ¬å–Redditå­ç‰ˆå— - ä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„JSON APIæ–¹æ³•"""
        posts = []
        
        try:
            logger.info(f"ğŸš€ çˆ¬å–Redditå­ç‰ˆå—: {subreddit_url}")
            
            # ä»URLæå–subredditåç§°
            if '/r/' in subreddit_url:
                subreddit = subreddit_url.split('/r/')[-1].split('/')[0]
            else:
                logger.error(f"æ— æ•ˆçš„Reddit URL: {subreddit_url}")
                return []
            
            # æ„é€ JSON API URLï¼ˆå·²éªŒè¯æœ‰æ•ˆï¼‰
            json_url = f"https://www.reddit.com/r/{subreddit}/new.json?limit=25"
            logger.info(f"ğŸ“¡ ä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„JSON API: {json_url}")
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive'
            }
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            import random
            time.sleep(random.uniform(2, 4))
            
            # å‘é€è¯·æ±‚
            response = self.session.get(json_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                logger.warning(f"Reddit APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
            
            # è§£æJSONå“åº”
            try:
                data = response.json()
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return []
            
            # æ£€æŸ¥æ•°æ®ç»“æ„
            if 'data' not in data or 'children' not in data['data']:
                logger.warning("Reddit APIè¿”å›æ•°æ®ç»“æ„å¼‚å¸¸")
                return []
            
            # 24å°æ—¶å‰çš„æ—¶é—´æˆ³
            from datetime import datetime, timedelta
            twenty_four_hours_ago = datetime.now() - timedelta(hours=24)
            twenty_four_hours_timestamp = twenty_four_hours_ago.timestamp()
            
            # å¤„ç†å¸–å­æ•°æ®
            for item in data['data']['children']:
                try:
                    post_data = item['data']
                    
                    # æ£€æŸ¥æ—¶é—´ï¼ˆUTCæ—¶é—´æˆ³ï¼‰
                    created_utc = post_data.get('created_utc', 0)
                    if created_utc < twenty_four_hours_timestamp:
                        continue  # è·³è¿‡24å°æ—¶å‰çš„å¸–å­
                    
                    # æå–å¸–å­ä¿¡æ¯
                    title = post_data.get('title', '').strip()
                    if not title:
                        continue
                    
                    # å…³é”®è¯è¿‡æ»¤
                    if keywords:
                        if not any(keyword.lower() in title.lower() for keyword in keywords):
                            continue
                    
                    # æ„é€ å¸–å­å¯¹è±¡
                    post = CompetitorPost()
                    post.title = title
                    post.content = post_data.get('selftext', '')
                    post.author = post_data.get('author', 'unknown')
                    post.post_time = datetime.fromtimestamp(created_utc)
                    post.post_url = f"https://www.reddit.com{post_data.get('permalink', '')}"
                    post.likes_count = post_data.get('score', 0)
                    post.comments_count = post_data.get('num_comments', 0)
                    post.platform = 'Reddit'
                    
                    posts.append(post)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†å¸–å­æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"âœ… Redditçˆ¬å–æˆåŠŸ: æ€»è®¡ {len(data['data']['children'])} æ¡ï¼Œ24å°æ—¶å†… {len(posts)} æ¡")
            return posts
            
        except Exception as e:
            logger.error(f"çˆ¬å–Redditå­ç‰ˆå—å¤±è´¥: {e}")
            return []
    
    def _crawl_reddit_json_api(self, subreddit_url: str, keywords: List[str]) -> List[CompetitorPost]:
        """ä½¿ç”¨Reddit JSON APIçˆ¬å–"""
        posts = []
        
        try:
            # æ„é€ JSON API URL
            # ä¾‹å¦‚: https://www.reddit.com/r/xToolOfficial/ -> https://www.reddit.com/r/xToolOfficial/new.json
            base_url = subreddit_url.rstrip('/')
            json_url = base_url + '/new.json?limit=25'
            
            # è®¾ç½®è¯·æ±‚å¤´æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
                'X-Requested-With': 'XMLHttpRequest'
            }
            
            # å‘é€è¯·æ±‚
            response = self.session.get(json_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›æœ‰æ•ˆJSON
            try:
                data = response.json()
            except:
                logger.warning("å“åº”ä¸æ˜¯æœ‰æ•ˆJSONï¼Œå¯èƒ½è¢«é‡å®šå‘æˆ–é˜»æ­¢")
                return posts
            
            # æ£€æŸ¥æ•°æ®ç»“æ„
            if not isinstance(data, dict) or 'data' not in data:
                logger.warning("JSONå“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                return posts
            
            children = data.get('data', {}).get('children', [])
            logger.info(f"ä»JSON APIè·å–åˆ° {len(children)} ä¸ªåŸå§‹å¸–å­")
            
            # å¤„ç†æ¯ä¸ªå¸–å­
            for item in children:
                try:
                    post_data = item.get('data', {})
                    if not post_data:
                        continue
                    
                    # æ£€æŸ¥æ—¶é—´é™åˆ¶ï¼ˆ24å°æ—¶å†…ï¼‰
                    created_utc = post_data.get('created_utc', 0)
                    if not self._is_within_24_hours(created_utc):
                        continue
                    
                    # æå–å¸–å­ä¿¡æ¯
                    title = post_data.get('title', '').strip()
                    if not title:  # è·³è¿‡æ²¡æœ‰æ ‡é¢˜çš„å¸–å­
                        continue
                    
                    content = post_data.get('selftext', '').strip()
                    author = post_data.get('author', '')
                    score = post_data.get('score', 0)
                    num_comments = post_data.get('num_comments', 0)
                    permalink = post_data.get('permalink', '')
                    
                    # æ„å»ºå®Œæ•´URL
                    post_url = f"https://www.reddit.com{permalink}" if permalink else subreddit_url
                    
                    # å…³é”®è¯è¿‡æ»¤
                    if keywords:
                        content_text = (title + ' ' + content).lower()
                        if not any(keyword.lower() in content_text for keyword in keywords):
                            continue
                    
                    # åˆ›å»ºå¸–å­å¯¹è±¡
                    post = CompetitorPost()
                    post.title = title
                    post.content = content[:500] if content else ""
                    post.author = author
                    post.post_time = datetime.fromtimestamp(created_utc) if created_utc > 0 else datetime.now()
                    post.post_url = post_url
                    post.likes_count = score if score > 0 else 0
                    post.comments_count = num_comments if num_comments > 0 else 0
                    post.platform = "Reddit"
                    
                    posts.append(post)
                    logger.debug(f"æˆåŠŸè§£æå¸–å­: {title[:50]}")
                    
                except Exception as e:
                    logger.debug(f"è§£æå•ä¸ªå¸–å­å¤±è´¥: {e}")
                    continue
            
            logger.info(f"JSON APIæœ€ç»ˆè·å– {len(posts)} æ¡æœ‰æ•ˆå¸–å­")
            return posts
            
        except Exception as e:
            logger.warning(f"Reddit JSON APIçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def _crawl_reddit_selenium(self, subreddit_url: str, keywords: List[str]) -> List[CompetitorPost]:
        """ä½¿ç”¨Seleniumçˆ¬å–Redditï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        posts = []
        
        try:
            # æ„é€ æ–°å¸–å­URL  
            if not subreddit_url.endswith('/'):
                subreddit_url += '/'
            new_posts_url = subreddit_url + 'new/'
            
            # å…ˆå°è¯•æ— å¤´æµè§ˆå™¨æ–¹å¼
            driver = self._init_webdriver()
            if not driver:
                logger.warning("WebDriveråˆå§‹åŒ–å¤±è´¥")
                return posts
            
            try:
                driver.get(new_posts_url)
                time.sleep(5)  # ç­‰å¾…é¡µé¢åŠ è½½
                
                # æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢
                page_source = driver.page_source
                if "blocked by network security" in page_source.lower() or "you've been blocked" in page_source.lower():
                    logger.warning("Redditè®¿é—®è¢«ç½‘ç»œå®‰å…¨é˜»æ­¢")
                    with open('reddit_debug.html', 'w', encoding='utf-8') as f:
                        f.write(page_source)
                    logger.info("è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ° reddit_debug.html")
                    return posts
                
                # å°è¯•å¤šç§å¯èƒ½çš„å¸–å­é€‰æ‹©å™¨
                post_selectors = [
                    'shreddit-post',  # æ–°ç‰ˆReddit
                    '[data-testid="post-container"]',  # ä¸­ç‰ˆReddit 
                    'div[data-click-id="body"]',  # æ—§ç‰ˆReddit
                    '.Post',  # å¤‡ç”¨é€‰æ‹©å™¨
                    'article'  # é€šç”¨æ–‡ç« æ ‡ç­¾
                ]
                
                post_elements = []
                for selector in post_selectors:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(elements)} ä¸ªå¸–å­")
                        post_elements = elements
                        break
                
                if not post_elements:
                    logger.warning("æœªæ‰¾åˆ°ä»»ä½•å¸–å­å…ƒç´ ")
                    with open('reddit_debug.html', 'w', encoding='utf-8') as f:
                        f.write(page_source)
                    logger.info("é¡µé¢æºç å·²ä¿å­˜åˆ° reddit_debug.html")
                    return posts
                
                for element in post_elements[:20]:  # é™åˆ¶æœ€å¤šå¤„ç†20ä¸ªå¸–å­
                    try:
                        post = CompetitorPost()
                        post.platform = "Reddit"
                        
                        # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
                        title_selectors = [
                            'h3 a[slot="title"]',
                            'h3 a',
                            '[data-testid="post-content"] h3',
                            '.title a',
                            'a[data-click-id="title"]',
                            'shreddit-post h3'
                        ]
                        
                        title_found = False
                        for title_selector in title_selectors:
                            try:
                                title_element = element.find_element(By.CSS_SELECTOR, title_selector)
                                if title_element:
                                    post.title = title_element.text.strip()
                                    post.post_url = title_element.get_attribute('href') or ''
                                    title_found = True
                                    break
                            except:
                                continue
                        
                        if not title_found:
                            # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜é“¾æ¥ï¼Œå°è¯•ç›´æ¥æ‰¾æ ‡é¢˜æ–‡æœ¬
                            try:
                                title_element = element.find_element(By.TAG_NAME, 'h3')
                                post.title = title_element.text.strip()
                            except:
                                continue
                        
                        # æå–ä½œè€…
                        author_selectors = [
                            'shreddit-post-author-line a',
                            '[data-testid="post_author_link"]',
                            '.author',
                            'a[href*="/user/"]'
                        ]
                        
                        for author_selector in author_selectors:
                            try:
                                author_element = element.find_element(By.CSS_SELECTOR, author_selector)
                                post.author = author_element.text.strip().replace('u/', '')
                                break
                            except:
                                continue
                        
                        # æå–æ—¶é—´
                        time_selectors = [
                            'shreddit-post-author-line time',
                            'time',
                            '[data-testid="post_timestamp"]',
                            '.live-timestamp'
                        ]
                        
                        post_time_found = False
                        for time_selector in time_selectors:
                            try:
                                time_element = element.find_element(By.CSS_SELECTOR, time_selector)
                                time_str = time_element.get_attribute('datetime') or time_element.get_attribute('title') or time_element.text
                                post.post_time = self._parse_reddit_time(time_str)
                                post_time_found = True
                                break
                            except:
                                continue
                        
                        if not post_time_found:
                            post.post_time = datetime.now()
                        
                        # æå–å†…å®¹é¢„è§ˆ
                        try:
                            content_element = element.find_element(By.CSS_SELECTOR, '[data-testid="post-content"] div, .usertext-body, p')
                            post.content = content_element.text.strip()[:500]
                        except:
                            post.content = ""
                        
                        # å…³é”®è¯è¿‡æ»¤
                        if keywords:
                            content_text = (post.title + ' ' + post.content).lower()
                            if not any(keyword.lower() in content_text for keyword in keywords):
                                continue
                        
                        if post.title:
                            posts.append(post)
                            logger.debug(f"æˆåŠŸæå–å¸–å­: {post.title[:50]}")
                            
                    except Exception as e:
                        logger.debug(f"è§£æRedditå¸–å­å¤±è´¥: {e}")
                        continue
                
                driver.quit()
                
            except Exception as e:
                logger.error(f"Seleniumçˆ¬å–Redditå¤±è´¥: {e}")
                if driver:
                    driver.quit()
                # é™çº§åˆ°é™æ€è¯·æ±‚
                return self._crawl_reddit_static(new_posts_url, keywords)
            
        except Exception as e:
            logger.error(f"çˆ¬å–Redditå­ç‰ˆå—å¤±è´¥: {e}")
        
        logger.info(f"Redditçˆ¬å–å®Œæˆ: {len(posts)} æ¡å¸–å­")
        return posts
    
    def _crawl_reddit_static(self, url: str, keywords: List[str]) -> List[CompetitorPost]:
        """é™æ€æ–¹å¼çˆ¬å–Redditï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        posts = []
        
        try:
            # å°è¯•ä¸åŒçš„User-Agent
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            response = self.session.get(url, headers=headers)
            response.raise_for_status()
            
            # å¦‚æœè¿”å›çš„æ˜¯ç™»å½•é¡µé¢ï¼Œè¯´æ˜è¢«é‡å®šå‘äº†
            if 'login' in response.url or 'sign_in' in response.url:
                logger.warning("è¢«é‡å®šå‘åˆ°ç™»å½•é¡µé¢ï¼Œæ— æ³•è®¿é—®å†…å®¹")
                return posts
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æŸ¥æ‰¾æ—§ç‰ˆRedditçš„å¸–å­
            post_elements = soup.select('.thing, .Post, div[data-domain]')
            
            for element in post_elements[:20]:
                try:
                    post = CompetitorPost()
                    post.platform = "Reddit"
                    
                    # æå–æ ‡é¢˜
                    title_element = element.select_one('.title a, h3 a, a.title')
                    if title_element:
                        post.title = title_element.get_text(strip=True)
                        href = title_element.get('href', '')
                        if href.startswith('/'):
                            post.post_url = 'https://www.reddit.com' + href
                        else:
                            post.post_url = href
                    
                    # æå–ä½œè€…
                    author_element = element.select_one('.author, [data-author]')
                    if author_element:
                        post.author = author_element.get_text(strip=True)
                    
                    # è®¾ç½®æ—¶é—´ä¸ºå½“å‰æ—¶é—´ï¼ˆé™æ€æ–¹å¼éš¾ä»¥å‡†ç¡®è·å–ï¼‰
                    post.post_time = datetime.now()
                    
                    if post.title:
                        posts.append(post)
                        
                except Exception as e:
                    logger.debug(f"è§£æé™æ€Redditå¸–å­å¤±è´¥: {e}")
                    continue
            
            logger.info(f"é™æ€æ–¹å¼çˆ¬å–Reddit: {len(posts)} æ¡")
            
        except Exception as e:
            logger.error(f"é™æ€çˆ¬å–Redditå¤±è´¥: {e}")
        
        return posts
    
    def _crawl_reddit_user(self, user_url: str) -> List[CompetitorPost]:
        """çˆ¬å–Redditç”¨æˆ·é¡µé¢"""
        posts = []
        
        try:
            logger.info(f"çˆ¬å–Redditç”¨æˆ·: {user_url}")
            
            response = self.session.get(user_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾ç”¨æˆ·å‘å¸ƒçš„å¸–å­
            post_elements = soup.select('[data-testid="post-container"], .thing')
            
            for element in post_elements:
                try:
                    post = CompetitorPost()
                    post.platform = "Reddit"
                    
                    # æå–æ ‡é¢˜
                    title_element = element.select_one('h3 a, .title a')
                    if title_element:
                        post.title = title_element.get_text(strip=True)
                        post.post_url = title_element.get('href', '')
                    
                    # æå–ä½œè€…ï¼ˆä»ç”¨æˆ·é¡µé¢çˆ¬å–ï¼Œä½œè€…å°±æ˜¯è¯¥ç”¨æˆ·ï¼‰
                    post.author = user_url.split('/')[-1] if user_url.endswith('/') else user_url.split('/')[-1]
                    
                    # æå–æ—¶é—´
                    post.post_time = datetime.now()  # ç®€åŒ–å¤„ç†
                    
                    if post.title:
                        posts.append(post)
                        
                except Exception as e:
                    logger.debug(f"è§£æRedditç”¨æˆ·å¸–å­å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"çˆ¬å–Redditç”¨æˆ·å¤±è´¥: {e}")
        
        return posts
    
    def _crawl_kickstarter_search(self, website_url: str, keywords: List[str]) -> List[CompetitorPost]:
        """çˆ¬å–Kickstarteræœç´¢ç»“æœ"""
        posts = []
        
        for keyword in keywords:
            try:
                search_url = f"https://www.kickstarter.com/discover/advanced?term={keyword}&sort=newest"
                logger.info(f"æœç´¢Kickstarterå…³é”®è¯: {keyword}")
                
                response = self.session.get(search_url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾é¡¹ç›®åˆ—è¡¨
                project_elements = soup.select('.project-card, .js-react-on-rails-component')
                
                for element in project_elements:
                    try:
                        post = CompetitorPost()
                        post.platform = "Kickstarter"
                        
                        # æå–é¡¹ç›®ä¿¡æ¯
                        title_element = element.select_one('h3 a, .project-title a')
                        if title_element:
                            post.title = title_element.get_text(strip=True)
                            post.post_url = title_element.get('href', '')
                        
                        post.post_time = datetime.now()
                        
                        if post.title:
                            posts.append(post)
                            
                    except Exception as e:
                        logger.debug(f"è§£æKickstarteræœç´¢ç»“æœå¤±è´¥: {e}")
                        continue
                
            except Exception as e:
                logger.error(f"æœç´¢Kickstarterå…³é”®è¯å¤±è´¥ {keyword}: {e}")
        
        return posts
    
    def _parse_reddit_time(self, time_str: str) -> datetime:
        """è§£æRedditæ—¶é—´"""
        try:
            # å°è¯•ISOæ ¼å¼
            if 'T' in time_str:
                return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
            
            # ç›¸å¯¹æ—¶é—´è§£æ
            now = datetime.now()
            time_str = time_str.lower()
            
            if 'minute' in time_str:
                minutes = int(re.findall(r'(\d+)', time_str)[0])
                return now - timedelta(minutes=minutes)
            elif 'hour' in time_str:
                hours = int(re.findall(r'(\d+)', time_str)[0])
                return now - timedelta(hours=hours)
            elif 'day' in time_str:
                days = int(re.findall(r'(\d+)', time_str)[0])
                return now - timedelta(days=days)
            
        except Exception as e:
            logger.debug(f"è§£ææ—¶é—´å¤±è´¥: {time_str}, {e}")
        
        return datetime.now()
    
    def _init_webdriver(self) -> webdriver.Chrome:
        """åˆå§‹åŒ–WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(30)
            return self.driver
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–WebDriverå¤±è´¥: {e}")
            return None
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.driver:
            self.driver.quit()
            self.driver = None
    
    def _is_within_24_hours(self, timestamp):
        """æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨24å°æ—¶å†…"""
        try:
            if isinstance(timestamp, (int, float)):
                post_time = datetime.fromtimestamp(timestamp)
            elif isinstance(timestamp, str):
                # å°è¯•è§£ææ—¶é—´å­—ç¬¦ä¸²
                post_time = self._parse_reddit_time(timestamp)
            else:
                return True  # å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤åŒ…å«
            
            now = datetime.now()
            time_diff = now - post_time
            return time_diff.days == 0 and time_diff.seconds < 86400  # 24å°æ—¶ = 86400ç§’
        except:
            return True
    
    def _crawl_reddit_html_direct(self, subreddit_url: str, keywords: List[str]) -> List[CompetitorPost]:
        """ç›´æ¥HTMLè§£æReddité¡µé¢ï¼ˆç»•è¿‡APIé™åˆ¶ï¼‰"""
        posts = []
        
        try:
            # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
            if not subreddit_url.endswith('/'):
                subreddit_url += '/'
            new_url = subreddit_url + 'new/'
            
            logger.info(f"ä½¿ç”¨HTMLæ–¹å¼çˆ¬å–Reddit: {new_url}")
            
            # è®¾ç½®é€¼çœŸçš„æµè§ˆå™¨å¤´éƒ¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,zh;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            import random
            time.sleep(random.uniform(3, 6))
            
            # å‘é€è¯·æ±‚
            response = self.session.get(new_url, headers=headers, timeout=30)
            
            # æ£€æŸ¥å“åº”
            if response.status_code != 200:
                logger.warning(f"Reddit HTMLè®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return posts
            
            html_content = response.text
            
            # æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢
            if 'blocked by network security' in html_content.lower():
                logger.error("Reddit HTMLè®¿é—®ä¹Ÿè¢«é˜»æ­¢")
                return posts
            
            # ä¿å­˜è°ƒè¯•ä¿¡æ¯
            try:
                with open('reddit_html_debug.html', 'w', encoding='utf-8') as f:
                    f.write(html_content[:5000])  # åªä¿å­˜å‰5000å­—ç¬¦
                logger.info("HTMLè°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ° reddit_html_debug.html")
            except:
                pass
            
            # ä½¿ç”¨BeautifulSoupè§£æ
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # å°è¯•å¤šç§å¯èƒ½çš„å¸–å­é€‰æ‹©å™¨ï¼ˆRedditç»å¸¸æ”¹å˜ç»“æ„ï¼‰
            post_selectors = [
                'shreddit-post',  # æ–°ç‰ˆReddit
                '[data-testid="post-container"]',  # ä¸­ç‰ˆReddit
                '.Post',  # æ—§ç‰ˆReddit
                'article',  # é€šç”¨
                '[data-click-id="body"]',  # å¦ä¸€ç§å¯èƒ½
                '.thing',  # æ›´æ—§çš„ç‰ˆæœ¬
            ]
            
            post_elements = []
            for selector in post_selectors:
                post_elements = soup.select(selector)
                if post_elements:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ°å¸–å­: {selector}, æ•°é‡: {len(post_elements)}")
                    break
            
            if not post_elements:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•å¸–å­å…ƒç´ ï¼Œå°è¯•å¯»æ‰¾é“¾æ¥")
                # å°è¯•å¯»æ‰¾Reddité£æ ¼çš„é“¾æ¥
                link_elements = soup.find_all('a', href=True)
                post_links = []
                for link in link_elements:
                    href = link.get('href', '')
                    if '/comments/' in href and href.startswith(('/r/', 'https://www.reddit.com/r/')):
                        post_links.append({
                            'title': link.get_text(strip=True),
                            'url': href if href.startswith('http') else f"https://www.reddit.com{href}"
                        })
                
                logger.info(f"æ‰¾åˆ° {len(post_links)} ä¸ªå¸–å­é“¾æ¥")
                
                # ä¸ºæ¯ä¸ªé“¾æ¥åˆ›å»ºåŸºæœ¬å¸–å­å¯¹è±¡
                for link_data in post_links[:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                    if link_data['title'] and len(link_data['title']) > 5:
                        post = CompetitorPost()
                        post.title = link_data['title']
                        post.content = ""
                        post.author = "unknown"
                        post.post_time = datetime.now()
                        post.post_url = link_data['url']
                        post.likes_count = 0
                        post.comments_count = 0
                        post.platform = "Reddit"
                        
                        # å…³é”®è¯æ£€æŸ¥
                        if keywords:
                            if any(keyword.lower() in post.title.lower() for keyword in keywords):
                                posts.append(post)
                        else:
                            posts.append(post)
                
                logger.info(f"HTMLé“¾æ¥æ–¹å¼è·å– {len(posts)} æ¡å¸–å­")
                return posts
            
            # è§£ææ‰¾åˆ°çš„å¸–å­å…ƒç´ 
            for element in post_elements[:15]:  # é™åˆ¶å¤„ç†æ•°é‡
                try:
                    # æå–æ ‡é¢˜
                    title = ""
                    title_selectors = [
                        '[slot="title"]',
                        'h3',
                        '.title',
                        '[data-testid="post-content"] h3'
                    ]
                    
                    for title_sel in title_selectors:
                        title_elem = element.select_one(title_sel)
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            break
                    
                    if not title or len(title) < 3:
                        continue
                    
                    # æå–é“¾æ¥
                    url = ""
                    link_elem = element.find('a', href=True)
                    if link_elem:
                        href = link_elem.get('href', '')
                        if href.startswith('/'):
                            url = f"https://www.reddit.com{href}"
                        elif href.startswith('http'):
                            url = href
                    
                    # æå–ä½œè€…ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                    author = "unknown"
                    author_selectors = ['[slot="authorName"]', '.author', '[data-testid="comment_author_link"]']
                    for auth_sel in author_selectors:
                        auth_elem = element.select_one(auth_sel)
                        if auth_elem:
                            author = auth_elem.get_text(strip=True)
                            break
                    
                    # åˆ›å»ºå¸–å­å¯¹è±¡
                    post = CompetitorPost()
                    post.title = title
                    post.content = ""
                    post.author = author
                    post.post_time = datetime.now()  # HTMLè§£æéš¾ä»¥è·å–ç²¾ç¡®æ—¶é—´
                    post.post_url = url if url else subreddit_url
                    post.likes_count = 0
                    post.comments_count = 0
                    post.platform = "Reddit"
                    
                    # å…³é”®è¯è¿‡æ»¤
                    if keywords:
                        if any(keyword.lower() in title.lower() for keyword in keywords):
                            posts.append(post)
                    else:
                        posts.append(post)
                    
                    logger.debug(f"HTMLè§£æè·å–å¸–å­: {title[:50]}")
                    
                except Exception as e:
                    logger.debug(f"è§£æHTMLå¸–å­å…ƒç´ å¤±è´¥: {e}")
                    continue
            
            logger.info(f"HTMLè§£ææœ€ç»ˆè·å– {len(posts)} æ¡å¸–å­")
            return posts
            
        except Exception as e:
            logger.error(f"Reddit HTMLè§£æå¤±è´¥: {e}")
            return []
    
    def crawl_webpage_updates(self, config: dict) -> List[CompetitorPost]:
        """ç½‘é¡µæ›´æ–°ç›‘æ§æ¨¡å¼ - æ™ºèƒ½å·®å¼‚æ£€æµ‹ï¼ŒåªæŠ¥å‘Šæ›´æ–°éƒ¨åˆ†"""
        posts = []
        webpage_url = config.get('webpage_url')
        stored_hash = config.get('content_hash')
        last_content = config.get('last_content', '')
        
        if not webpage_url:
            logger.warning("ç½‘é¡µæ›´æ–°æ¨¡å¼ç¼ºå°‘webpage_urlé…ç½®")
            return posts
        
        try:
            logger.info(f"ğŸ” æ™ºèƒ½æ£€æµ‹ç½‘é¡µæ›´æ–°: {webpage_url}")
            
            # è·å–ç½‘é¡µå†…å®¹
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = self.session.get(webpage_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                logger.warning(f"ç½‘é¡µè®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return posts
            
            # æå–å¹¶æ¸…ç†ç½‘é¡µå†…å®¹
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸ç›¸å…³çš„æ ‡ç­¾
            for tag in soup(["script", "style", "nav", "footer", "header", "sidebar", "advertisement"]):
                tag.decompose()
            
            # è·å–æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
            current_content = soup.get_text(separator='\n', strip=True)
            
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            import hashlib
            current_hash = hashlib.md5(current_content.encode('utf-8')).hexdigest()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°
            if stored_hash and current_hash == stored_hash:
                logger.info("ğŸ“‹ ç½‘é¡µå†…å®¹æ— æ›´æ–°")
                return posts
            
            # æå–é¡µé¢æ ‡é¢˜
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ç½‘é¡µæ›´æ–°"
            
            if not last_content:
                # é¦–æ¬¡çˆ¬å–ï¼Œè®°å½•å…¨éƒ¨å†…å®¹
                logger.success(f"ğŸ†• é¦–æ¬¡çˆ¬å–ç½‘é¡µ: {title_text}")
                
                post = CompetitorPost()
                post.title = f"ğŸ“‹ {title_text} (é¦–æ¬¡å»ºç«‹ç›‘æ§)"
                post.content = current_content[:2000] + "..." if len(current_content) > 2000 else current_content
                post.author = "ç½‘é¡µç›‘æ§"
                post.post_time = datetime.now()
                post.post_url = webpage_url
                post.platform = "ç½‘é¡µæ›´æ–°"
                post.likes_count = 0
                post.comments_count = 0
                
                posts.append(post)
                
            else:
                # æ£€æµ‹å…·ä½“æ›´æ–°å†…å®¹
                logger.success(f"ğŸ†• æ£€æµ‹åˆ°ç½‘é¡µæ›´æ–°ï¼Œåˆ†æå·®å¼‚: {title_text}")
                
                # è·å–æ›´æ–°çš„å…·ä½“å†…å®¹
                updated_parts = self._extract_content_differences(last_content, current_content, soup)
                
                if updated_parts:
                    # æ£€æŸ¥æ›´æ–°éƒ¨åˆ†æ˜¯å¦åŒ…å«é“¾æ¥ï¼Œå¹¶çˆ¬å–é“¾æ¥å†…å®¹
                    enriched_content = self._enrich_updates_with_links(updated_parts, webpage_url)
                    
                    post = CompetitorPost()
                    post.title = f"ğŸ”„ {title_text} (å†…å®¹æ›´æ–°)"
                    post.content = enriched_content[:2000] + "..." if len(enriched_content) > 2000 else enriched_content
                    post.author = "ç½‘é¡µç›‘æ§"
                    post.post_time = datetime.now()
                    post.post_url = webpage_url
                    post.platform = "ç½‘é¡µæ›´æ–°"
                    post.likes_count = 0
                    post.comments_count = 0
                    
                    posts.append(post)
                    logger.success(f"ğŸ“„ å‘ç°æ›´æ–°å†…å®¹ï¼Œé•¿åº¦: {len(enriched_content)} å­—ç¬¦")
                else:
                    logger.info("ğŸ“‹ è™½ç„¶å“ˆå¸Œå€¼å˜åŒ–ï¼Œä½†æœªå‘ç°æ˜æ˜¾çš„å†…å®¹æ›´æ–°")
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„å“ˆå¸Œå€¼å’Œå†…å®¹
            self._update_webpage_data(config['id'], current_hash, current_content)
            
            logger.success(f"âœ… æ™ºèƒ½ç½‘é¡µæ›´æ–°ç›‘æ§å®Œæˆï¼Œè·å– {len(posts)} æ¡æ›´æ–°")
            
        except Exception as e:
            logger.error(f"âŒ ç½‘é¡µæ›´æ–°ç›‘æ§å¤±è´¥: {e}")
        
        return posts
    
    def _extract_content_differences(self, old_content: str, new_content: str, soup: BeautifulSoup) -> str:
        """æå–ç½‘é¡µå†…å®¹çš„å…·ä½“å·®å¼‚éƒ¨åˆ†"""
        try:
            import difflib
            
            # æŒ‰è¡Œåˆ†å‰²å†…å®¹è¿›è¡Œå¯¹æ¯”
            old_lines = old_content.split('\n')
            new_lines = new_content.split('\n')
            
            # ä½¿ç”¨difflibæ‰¾å‡ºæ–°å¢çš„è¡Œ
            differ = difflib.unified_diff(old_lines, new_lines, lineterm='', n=0)
            diff_lines = list(differ)
            
            # æå–æ–°å¢çš„å†…å®¹è¡Œ
            added_lines = []
            for line in diff_lines:
                if line.startswith('+ ') or line.startswith('+'):
                    clean_line = line[1:].strip() if line.startswith('+ ') else line[1:].strip()
                    if clean_line and len(clean_line) > 3:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¡Œ
                        added_lines.append(clean_line)
            
            if not added_lines:
                # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æ–°å¢è¡Œï¼Œå°è¯•æ‰¾åˆ°å˜åŒ–çš„æ®µè½
                added_lines = self._find_content_blocks_differences(old_content, new_content, soup)
            
            # ç»„åˆæ–°å¢å†…å®¹
            if added_lines:
                updated_content = '\n'.join(added_lines)
                logger.info(f"ğŸ“„ æ£€æµ‹åˆ° {len(added_lines)} è¡Œæ–°å¢å†…å®¹")
                return updated_content
            else:
                logger.info("ğŸ“‹ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å†…å®¹å·®å¼‚")
                return ""
                
        except Exception as e:
            logger.error(f"å†…å®¹å·®å¼‚åˆ†æå¤±è´¥: {e}")
            return ""
    
    def _find_content_blocks_differences(self, old_content: str, new_content: str, soup: BeautifulSoup) -> List[str]:
        """å¯»æ‰¾å†…å®¹å—çº§åˆ«çš„å·®å¼‚"""
        try:
            # å°è¯•ä»HTMLç»“æ„ä¸­æå–æ–°çš„å†…å®¹å—
            # æŸ¥æ‰¾å¸¸è§çš„å†…å®¹å—æ ‡ç­¾
            content_blocks = []
            
            for tag in soup.find_all(['article', 'section', 'div', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                block_text = tag.get_text(strip=True)
                if block_text and len(block_text) > 20:  # åªè€ƒè™‘æœ‰è¶³å¤Ÿå†…å®¹çš„å—
                    # æ£€æŸ¥è¿™ä¸ªå—æ˜¯å¦åœ¨æ—§å†…å®¹ä¸­å­˜åœ¨
                    if block_text not in old_content:
                        content_blocks.append(block_text)
            
            return content_blocks
            
        except Exception as e:
            logger.debug(f"å†…å®¹å—å·®å¼‚åˆ†æå¤±è´¥: {e}")
            return []
    
    def _enrich_updates_with_links(self, updated_content: str, base_url: str) -> str:
        """æ£€æŸ¥æ›´æ–°å†…å®¹ä¸­çš„é“¾æ¥å¹¶çˆ¬å–é“¾æ¥å†…å®¹"""
        try:
            import re
            from urllib.parse import urljoin, urlparse
            
            enriched_content = f"ğŸ”„ **æ›´æ–°å†…å®¹**:\n{updated_content}\n\n"
            
            # æå–é“¾æ¥ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            link_patterns = [
                r'https?://[^\s<>"]+',  # æ™®é€šHTTPé“¾æ¥
                r'www\.[^\s<>"]+',      # wwwå¼€å¤´çš„é“¾æ¥
            ]
            
            found_links = set()
            for pattern in link_patterns:
                links = re.findall(pattern, updated_content, re.IGNORECASE)
                for link in links:
                    # æ ‡å‡†åŒ–é“¾æ¥
                    if not link.startswith('http'):
                        link = 'https://' + link
                    
                    # éªŒè¯é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
                    try:
                        parsed = urlparse(link)
                        if parsed.netloc:
                            found_links.add(link)
                    except:
                        continue
            
            if found_links:
                logger.info(f"ğŸ”— åœ¨æ›´æ–°å†…å®¹ä¸­å‘ç° {len(found_links)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹çˆ¬å–")
                
                enriched_content += "ğŸ”— **ç›¸å…³é“¾æ¥å†…å®¹**:\n"
                
                for i, link in enumerate(list(found_links)[:3], 1):  # æœ€å¤šå¤„ç†3ä¸ªé“¾æ¥
                    try:
                        logger.info(f"ğŸ“ çˆ¬å–é“¾æ¥ {i}: {link}")
                        
                        # çˆ¬å–é“¾æ¥å†…å®¹
                        link_content = self._crawl_link_content(link)
                        
                        if link_content:
                            enriched_content += f"\n**é“¾æ¥ {i}**: {link}\n"
                            enriched_content += f"**å†…å®¹æ‘˜è¦**: {link_content[:500]}{'...' if len(link_content) > 500 else ''}\n"
                        else:
                            enriched_content += f"\n**é“¾æ¥ {i}**: {link} (æ— æ³•è·å–å†…å®¹)\n"
                            
                    except Exception as e:
                        logger.warning(f"çˆ¬å–é“¾æ¥å¤±è´¥ {link}: {e}")
                        enriched_content += f"\n**é“¾æ¥ {i}**: {link} (çˆ¬å–å¤±è´¥)\n"
                        continue
            
            return enriched_content
            
        except Exception as e:
            logger.error(f"é“¾æ¥å†…å®¹ä¸°å¯ŒåŒ–å¤±è´¥: {e}")
            return updated_content
    
    def _crawl_link_content(self, url: str) -> str:
        """çˆ¬å–å•ä¸ªé“¾æ¥çš„å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = self.session.get(url, headers=headers, timeout=15)
            
            if response.status_code != 200:
                return ""
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ä¸ç›¸å…³çš„æ ‡ç­¾
            for tag in soup(["script", "style", "nav", "footer", "header", "sidebar", "advertisement"]):
                tag.decompose()
            
            # æå–ä¸»è¦å†…å®¹
            content = soup.get_text(separator=' ', strip=True)
            
            # æ¸…ç†å’Œç¼©çŸ­å†…å®¹
            lines = content.split('\n')
            meaningful_lines = [line.strip() for line in lines if line.strip() and len(line.strip()) > 10]
            
            return '\n'.join(meaningful_lines[:20])  # æœ€å¤š20è¡Œ
            
        except Exception as e:
            logger.debug(f"é“¾æ¥å†…å®¹çˆ¬å–å¤±è´¥ {url}: {e}")
            return ""
    
    def _update_webpage_data(self, config_id: int, new_hash: str, new_content: str):
        """æ›´æ–°é…ç½®ä¸­çš„ç½‘é¡µæ•°æ®ï¼ˆå“ˆå¸Œå€¼å’Œå†…å®¹ï¼‰"""
        try:
            from models.competitor_models import db, MonitorConfig
            
            config = MonitorConfig.query.get(config_id)
            if config:
                config.content_hash = new_hash
                config.last_content = new_content
                db.session.commit()
                logger.debug(f"å·²æ›´æ–°é…ç½® {config_id} çš„ç½‘é¡µæ•°æ®")
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç½‘é¡µæ•°æ®å¤±è´¥: {e}") 