#!/usr/bin/env python3
"""
Facebookçˆ¬è™« - åŸºäºGitHubé¡¹ç›®ä¼˜åŒ–çš„å®ç°
ä½¿ç”¨Cookieå’ŒHeadersè¿›è¡Œèº«ä»½éªŒè¯ï¼Œæé«˜çˆ¬å–æˆåŠŸç‡
å‚è€ƒ: https://github.com/wz289494/Crawl_Facebook_User
"""

import re
import time
import json
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from loguru import logger

class FacebookPost:
    """Facebookå¸–å­æ•°æ®ç»“æ„"""
    def __init__(self):
        self.title: str = ""
        self.content: str = ""
        self.author: str = ""
        self.post_url: str = ""
        self.post_time: datetime = None
        self.likes_count: int = 0
        self.comments_count: int = 0
        self.shares_count: int = 0
        self.platform: str = "facebook"
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            "title": self.title,
            "content": self.content,
            "author": self.author,
            "post_url": self.post_url,
            "post_time": self.post_time.isoformat() if self.post_time else None,
            "likes_count": self.likes_count,
            "comments_count": self.comments_count,
            "shares_count": self.shares_count,
            "platform": self.platform
        }

class FacebookCrawler:
    """Facebookä¸“ç”¨çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        
        # åŸºäºGitHubé¡¹ç›®ä¼˜åŒ–çš„Headers
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        self.session.headers.update(self.headers)
        
        # å¯é€‰çš„Cookiesé…ç½®ï¼ˆéœ€è¦ç”¨æˆ·æä¾›ï¼‰
        self.cookies = {}
        
        # åŠ è½½é…ç½®
        self._load_config()
        
    def _load_config(self):
        """åŠ è½½Facebooké…ç½®"""
        try:
            # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
            import config
            if hasattr(config, 'FACEBOOK_COOKIES'):
                self.cookies.update(config.FACEBOOK_COOKIES)
                logger.info("âœ… åŠ è½½Facebook Cookieé…ç½®")
        except (ImportError, AttributeError):
            logger.warning("âš ï¸ æœªæ‰¾åˆ°Facebook Cookieé…ç½®ï¼Œå°†ä½¿ç”¨å…¬å¼€è®¿é—®æ¨¡å¼")
    
    def crawl_page(self, page_url: str, max_posts: int = 20) -> List[FacebookPost]:
        """çˆ¬å–Facebooké¡µé¢çš„å¸–å­"""
        posts = []
        
        try:
            logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–Facebooké¡µé¢: {page_url}")
            
            # æ–¹æ³•1: å°è¯•ç§»åŠ¨ç‰ˆFacebook (æ›´å®¹æ˜“çˆ¬å–)
            mobile_url = self._convert_to_mobile_url(page_url)
            posts_mobile = self._crawl_mobile_facebook(mobile_url, max_posts)
            
            if posts_mobile:
                logger.info(f"âœ… ç§»åŠ¨ç‰ˆçˆ¬å–æˆåŠŸï¼Œè·å¾— {len(posts_mobile)} æ¡å¸–å­")
                return posts_mobile
            
            # æ–¹æ³•2: å¦‚æœç§»åŠ¨ç‰ˆå¤±è´¥ï¼Œå°è¯•æ¡Œé¢ç‰ˆ
            logger.info("ğŸ“± ç§»åŠ¨ç‰ˆçˆ¬å–å¤±è´¥ï¼Œå°è¯•æ¡Œé¢ç‰ˆ...")
            posts_desktop = self._crawl_desktop_facebook(page_url, max_posts)
            
            if posts_desktop:
                logger.info(f"âœ… æ¡Œé¢ç‰ˆçˆ¬å–æˆåŠŸï¼Œè·å¾— {len(posts_desktop)} æ¡å¸–å­")
                return posts_desktop
            
            # æ–¹æ³•3: ä½¿ç”¨Seleniumä½œä¸ºæœ€åæ‰‹æ®µ
            logger.info("ğŸ’» æ¡Œé¢ç‰ˆçˆ¬å–å¤±è´¥ï¼Œå°è¯•Selenium...")
            posts_selenium = self._crawl_with_selenium(page_url, max_posts)
            
            if posts_selenium:
                logger.info(f"âœ… Seleniumçˆ¬å–æˆåŠŸï¼Œè·å¾— {len(posts_selenium)} æ¡å¸–å­")
                return posts_selenium
            
        except Exception as e:
            logger.error(f"âŒ Facebookçˆ¬å–å¼‚å¸¸: {e}")
        
        return posts
    
    def _convert_to_mobile_url(self, url: str) -> str:
        """è½¬æ¢ä¸ºç§»åŠ¨ç‰ˆFacebook URL"""
        if 'facebook.com' in url:
            # ä¿®å¤URLè½¬æ¢é€»è¾‘
            url = url.replace('www.facebook.com', 'm.facebook.com')
            if 'm.facebook.com' not in url:
                url = url.replace('facebook.com', 'm.facebook.com')
        return url
    
    def _crawl_mobile_facebook(self, url: str, max_posts: int) -> List[FacebookPost]:
        """çˆ¬å–ç§»åŠ¨ç‰ˆFacebook"""
        posts = []
        
        try:
            # æ›´æ–°Cookie
            if self.cookies:
                self.session.cookies.update(self.cookies)
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âš ï¸ ç§»åŠ¨ç‰ˆFacebookè®¿é—®å¤±è´¥: {response.status_code}")
                return posts
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾å¸–å­å®¹å™¨ (ç§»åŠ¨ç‰ˆFacebookçš„ç»“æ„)
            post_elements = soup.find_all('div', {'data-ft': True}) or soup.find_all('article')
            
            logger.info(f"ğŸ“± æ‰¾åˆ° {len(post_elements)} ä¸ªæ½œåœ¨å¸–å­å…ƒç´ ")
            
            for i, element in enumerate(post_elements[:max_posts]):
                try:
                    post = self._parse_mobile_post(element)
                    if post and self._is_recent_post(post.post_time):
                        posts.append(post)
                        logger.debug(f"âœ… è§£æå¸–å­ {i+1}: {post.title[:50]}...")
                except Exception as e:
                    logger.debug(f"âš ï¸ è§£æå¸–å­ {i+1} å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"âŒ ç§»åŠ¨ç‰ˆçˆ¬å–å¼‚å¸¸: {e}")
        
        return posts
    
    def _crawl_desktop_facebook(self, url: str, max_posts: int) -> List[FacebookPost]:
        """çˆ¬å–æ¡Œé¢ç‰ˆFacebook"""
        posts = []
        
        try:
            # æ›´æ–°Cookie
            if self.cookies:
                self.session.cookies.update(self.cookies)
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âš ï¸ æ¡Œé¢ç‰ˆFacebookè®¿é—®å¤±è´¥: {response.status_code}")
                return posts
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°ç™»å½•é¡µ
            if 'login.php' in response.url or 'checkpoint' in response.url:
                logger.warning("âš ï¸ éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®è¯¥Facebooké¡µé¢")
                return posts
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾å¸–å­å®¹å™¨ (æ¡Œé¢ç‰ˆFacebookçš„ç»“æ„)
            post_elements = (soup.find_all('div', {'data-pagelet': True}) or 
                           soup.find_all('div', class_=re.compile(r'.*story.*', re.I)) or
                           soup.find_all('div', {'role': 'article'}))
            
            logger.info(f"ğŸ’» æ‰¾åˆ° {len(post_elements)} ä¸ªæ½œåœ¨å¸–å­å…ƒç´ ")
            
            for i, element in enumerate(post_elements[:max_posts]):
                try:
                    post = self._parse_desktop_post(element)
                    if post and self._is_recent_post(post.post_time):
                        posts.append(post)
                        logger.debug(f"âœ… è§£æå¸–å­ {i+1}: {post.title[:50]}...")
                except Exception as e:
                    logger.debug(f"âš ï¸ è§£æå¸–å­ {i+1} å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"âŒ æ¡Œé¢ç‰ˆçˆ¬å–å¼‚å¸¸: {e}")
        
        return posts
    
    def _crawl_with_selenium(self, url: str, max_posts: int) -> List[FacebookPost]:
        """ä½¿ç”¨Seleniumçˆ¬å–Facebook"""
        posts = []
        driver = None
        
        try:
            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # è®¾ç½®User-Agent
            chrome_options.add_argument(f'--user-agent={self.headers["User-Agent"]}')
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # æ·»åŠ Cookie
            driver.get("https://www.facebook.com")
            for name, value in self.cookies.items():
                driver.add_cookie({'name': name, 'value': value})
            
            # è®¿é—®ç›®æ ‡é¡µé¢
            driver.get(url)
            time.sleep(3)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            self._scroll_and_wait(driver, max_scrolls=3)
            
            # æŸ¥æ‰¾å¸–å­å…ƒç´ 
            post_elements = driver.find_elements(By.CSS_SELECTOR, 
                "[data-pagelet*='FeedUnit'], [role='article'], div[data-ft]")
            
            logger.info(f"ğŸ¤– Seleniumæ‰¾åˆ° {len(post_elements)} ä¸ªæ½œåœ¨å¸–å­å…ƒç´ ")
            
            for i, element in enumerate(post_elements[:max_posts]):
                try:
                    post = self._parse_selenium_post(element)
                    if post and self._is_recent_post(post.post_time):
                        posts.append(post)
                        logger.debug(f"âœ… è§£æå¸–å­ {i+1}: {post.title[:50]}...")
                except Exception as e:
                    logger.debug(f"âš ï¸ è§£æå¸–å­ {i+1} å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"âŒ Seleniumçˆ¬å–å¼‚å¸¸: {e}")
        finally:
            if driver:
                driver.quit()
        
        return posts
    
    def _scroll_and_wait(self, driver, max_scrolls=3):
        """æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹"""
        for i in range(max_scrolls):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            logger.debug(f"ğŸ“œ æ»šåŠ¨ {i+1}/{max_scrolls}")
    
    def _parse_mobile_post(self, element) -> Optional[FacebookPost]:
        """è§£æç§»åŠ¨ç‰ˆFacebookå¸–å­"""
        post = FacebookPost()
        
        try:
            # æå–å†…å®¹æ–‡æœ¬
            content_elem = element.find('p') or element.find('div', string=True)
            if content_elem:
                post.content = content_elem.get_text(strip=True)
                post.title = post.content[:100] + "..." if len(post.content) > 100 else post.content
            
            # æå–ä½œè€…
            author_elem = element.find('h3') or element.find('strong')
            if author_elem:
                post.author = author_elem.get_text(strip=True)
            
            # æå–é“¾æ¥
            link_elem = element.find('a', href=True)
            if link_elem:
                post.post_url = urljoin('https://m.facebook.com', link_elem['href'])
            
            # è®¾ç½®æ—¶é—´ï¼ˆç§»åŠ¨ç‰ˆè¾ƒéš¾æå–å‡†ç¡®æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ï¼‰
            post.post_time = datetime.now()
            
            return post if post.content else None
            
        except Exception as e:
            logger.debug(f"ç§»åŠ¨ç‰ˆå¸–å­è§£æå¤±è´¥: {e}")
            return None
    
    def _parse_desktop_post(self, element) -> Optional[FacebookPost]:
        """è§£ææ¡Œé¢ç‰ˆFacebookå¸–å­"""
        post = FacebookPost()
        
        try:
            # æå–å†…å®¹æ–‡æœ¬
            content_selectors = [
                '[data-ad-preview="message"]',
                '[data-testid="post_message"]',
                'div[dir="auto"]',
                'p',
                'span'
            ]
            
            for selector in content_selectors:
                content_elem = element.select_one(selector)
                if content_elem and content_elem.get_text(strip=True):
                    post.content = content_elem.get_text(strip=True)
                    break
            
            if post.content:
                post.title = post.content[:100] + "..." if len(post.content) > 100 else post.content
            
            # æå–ä½œè€…
            author_selectors = [
                'h3[dir="auto"] a',
                'strong a',
                '[data-testid="story-subtitle"] a'
            ]
            
            for selector in author_selectors:
                author_elem = element.select_one(selector)
                if author_elem:
                    post.author = author_elem.get_text(strip=True)
                    break
            
            # æå–é“¾æ¥
            link_elem = element.find('a', href=True)
            if link_elem and 'href' in link_elem.attrs:
                post.post_url = urljoin('https://www.facebook.com', link_elem['href'])
            
            # è®¾ç½®æ—¶é—´
            post.post_time = datetime.now()
            
            return post if post.content else None
            
        except Exception as e:
            logger.debug(f"æ¡Œé¢ç‰ˆå¸–å­è§£æå¤±è´¥: {e}")
            return None
    
    def _parse_selenium_post(self, element) -> Optional[FacebookPost]:
        """è§£æSeleniumè·å–çš„Facebookå¸–å­"""
        post = FacebookPost()
        
        try:
            # æå–å†…å®¹æ–‡æœ¬
            try:
                post.content = element.find_element(By.CSS_SELECTOR, 
                    '[data-ad-preview="message"], [data-testid="post_message"], div[dir="auto"]').text
            except:
                post.content = element.text[:500] if element.text else ""
            
            if post.content:
                post.title = post.content[:100] + "..." if len(post.content) > 100 else post.content
            
            # æå–ä½œè€…
            try:
                post.author = element.find_element(By.CSS_SELECTOR, 
                    'h3[dir="auto"] a, strong a').text
            except:
                post.author = "Unknown"
            
            # æå–é“¾æ¥
            try:
                link_elem = element.find_element(By.TAG_NAME, 'a')
                post.post_url = link_elem.get_attribute('href')
            except:
                post.post_url = ""
            
            # è®¾ç½®æ—¶é—´
            post.post_time = datetime.now()
            
            return post if post.content else None
            
        except Exception as e:
            logger.debug(f"Seleniumå¸–å­è§£æå¤±è´¥: {e}")
            return None
    
    def _is_recent_post(self, post_time: datetime) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸º72å°æ—¶å†…çš„å¸–å­"""
        if not post_time:
            return True  # å¦‚æœæ— æ³•ç¡®å®šæ—¶é—´ï¼Œé»˜è®¤åŒ…å«
        
        now = datetime.now()
        return (now - post_time).total_seconds() <= 72 * 3600
    
    def test_crawl(self, page_url: str) -> Dict[str, Any]:
        """æµ‹è¯•Facebookçˆ¬å–åŠŸèƒ½"""
        logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•Facebookçˆ¬å–: {page_url}")
        
        start_time = time.time()
        posts = self.crawl_page(page_url, max_posts=5)
        end_time = time.time()
        
        result = {
            "success": len(posts) > 0,
            "posts_count": len(posts),
            "execution_time": round(end_time - start_time, 2),
            "posts": [post.to_dict() for post in posts],
            "page_url": page_url
        }
        
        if posts:
            logger.info(f"âœ… æµ‹è¯•æˆåŠŸï¼çˆ¬å–åˆ° {len(posts)} æ¡å¸–å­ï¼Œè€—æ—¶ {result['execution_time']}ç§’")
        else:
            logger.warning("âŒ æµ‹è¯•å¤±è´¥ï¼æœªèƒ½çˆ¬å–åˆ°ä»»ä½•å†…å®¹")
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    crawler = FacebookCrawler()
    
    # æµ‹è¯•çˆ¬å–Facebooké¡µé¢
    test_urls = [
        "https://www.facebook.com/LightBurnSoftware",
        "https://www.facebook.com/LaserMasterCorp",
        "https://www.facebook.com/xTool.tech"
    ]
    
    for url in test_urls:
        result = crawler.test_crawl(url)
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ - {url}")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"å¸–å­æ•°: {result['posts_count']}")
        print(f"è€—æ—¶: {result['execution_time']}ç§’")
        
        if result['posts']:
            print("ğŸ“ ç¤ºä¾‹å¸–å­:")
            for i, post in enumerate(result['posts'][:2]):
                print(f"  {i+1}. {post['title']}")
                print(f"     ä½œè€…: {post['author']}")
                print(f"     å†…å®¹: {post['content'][:100]}...")
        print("-" * 50) 