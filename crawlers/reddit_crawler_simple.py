#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç®€åŒ–çš„Redditçˆ¬è™« - åŸºäºåˆ†æç»“æœçš„æœ‰æ•ˆå®ç°
æ ¹æ®æµ‹è¯•ç»“æœï¼ŒJSON APIæ˜¯æœ€ä½³æ–¹æ³•
"""

import requests
import json
import time
import random
from datetime import datetime, timedelta
from typing import List
from loguru import logger

def crawl_reddit_simple(subreddit_url: str, keywords: List[str] = None) -> List[dict]:
    """
    ç®€åŒ–çš„Redditçˆ¬å–æ–¹æ³•
    åŸºäºåˆ†æç»“æœï¼Œä½¿ç”¨æœ€æœ‰æ•ˆçš„JSON APIæ–¹æ³•
    """
    posts = []
    
    try:
        # ä»URLæå–subredditåç§°
        if '/r/' in subreddit_url:
            subreddit = subreddit_url.split('/r/')[-1].split('/')[0]
        else:
            logger.error(f"æ— æ•ˆçš„Reddit URL: {subreddit_url}")
            return []
        
        # æ„é€ JSON API URL
        json_url = f"https://www.reddit.com/r/{subreddit}/new.json?limit=25"
        
        logger.info(f"ğŸš€ ä½¿ç”¨JSON APIçˆ¬å–: {json_url}")
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive'
        }
        
        # æ·»åŠ éšæœºå»¶è¿Ÿ
        time.sleep(random.uniform(2, 4))
        
        # å‘é€è¯·æ±‚
        response = requests.get(json_url, headers=headers, timeout=30)
        
        if response.status_code != 200:
            logger.warning(f"Reddit APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
        
        # è§£æJSONå“åº”
        try:
            data = response.json()
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return []
        
        # æ£€æŸ¥æ•°æ®ç»“æ„
        if 'data' not in data or 'children' not in data['data']:
            logger.warning("Reddit APIè¿”å›æ•°æ®ç»“æ„å¼‚å¸¸")
            return []
        
        # 72å°æ—¶å‰çš„æ—¶é—´æˆ³
        seventy_two_hours_ago = datetime.now() - timedelta(hours=72)
        seventy_two_hours_timestamp = seventy_two_hours_ago.timestamp()
        
        # å¤„ç†å¸–å­æ•°æ®
        for item in data['data']['children']:
            try:
                post_data = item['data']
                
                # æ£€æŸ¥æ—¶é—´ï¼ˆUTCæ—¶é—´æˆ³ï¼‰
                created_utc = post_data.get('created_utc', 0)
                if created_utc < seventy_two_hours_timestamp:
                    continue  # è·³è¿‡72å°æ—¶å‰çš„å¸–å­
                
                # æå–å¸–å­ä¿¡æ¯
                title = post_data.get('title', '').strip()
                if not title:
                    continue
                
                # å…³é”®è¯è¿‡æ»¤
                if keywords:
                    if not any(keyword.lower() in title.lower() for keyword in keywords):
                        continue
                
                # æ„é€ å¸–å­å¯¹è±¡
                post = {
                    'title': title,
                    'content': post_data.get('selftext', ''),
                    'author': post_data.get('author', 'unknown'),
                    'post_time': datetime.fromtimestamp(created_utc),
                    'post_url': f"https://www.reddit.com{post_data.get('permalink', '')}",
                    'likes_count': post_data.get('score', 0),
                    'comments_count': post_data.get('num_comments', 0),
                    'platform': 'Reddit'
                }
                
                posts.append(post)
                
            except Exception as e:
                logger.warning(f"å¤„ç†å¸–å­æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        logger.success(f"âœ… Redditçˆ¬å–æˆåŠŸ: æ€»è®¡ {len(data['data']['children'])} æ¡ï¼Œ24å°æ—¶å†… {len(posts)} æ¡")
        return posts
        
    except Exception as e:
        logger.error(f"âŒ Redditçˆ¬å–å¼‚å¸¸: {e}")
        return []

def test_reddit_crawler():
    """æµ‹è¯•Redditçˆ¬è™«"""
    print("ğŸ§ª æµ‹è¯•ç®€åŒ–Redditçˆ¬è™«")
    
    test_urls = [
        "https://www.reddit.com/r/xToolOfficial/",
        "https://www.reddit.com/r/WeCreat/"
    ]
    
    for url in test_urls:
        print(f"\nğŸ“ æµ‹è¯•URL: {url}")
        posts = crawl_reddit_simple(url)
        print(f"ğŸ“Š ç»“æœ: {len(posts)} æ¡å¸–å­")
        
        if posts:
            print("ğŸ“ å¸–å­é¢„è§ˆ:")
            for i, post in enumerate(posts[:3], 1):
                print(f"  {i}. {post['title'][:50]}...")

if __name__ == "__main__":
    test_reddit_crawler() 