#!/usr/bin/env python3
"""
ç«å“ç›‘æ§æ ¸å¿ƒæœåŠ¡
æ•´åˆçˆ¬è™«ã€AIåˆ†æã€æ•°æ®å­˜å‚¨ç­‰åŠŸèƒ½
"""

from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from models.competitor_models import db, MonitorConfig, CrawlSession, CompetitorPost
from crawlers.competitor_crawler import CompetitorCrawler
from services.competitor_ai_service import CompetitorAIService
from services.feishu_webhook_service import FeishuWebhookService
from loguru import logger
import hashlib

class CompetitorMonitorService:
    """ç«å“ç›‘æ§æ ¸å¿ƒæœåŠ¡"""
    
    def __init__(self):
        self.crawler = CompetitorCrawler()
        self.ai_service = CompetitorAIService()
        self.feishu_service = FeishuWebhookService()
    
    def execute_crawl_session(self, session_name: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬å–ä¼šè¯"""
        if not session_name:
            session_name = f"{datetime.now().strftime('%Y-%m-%d %H:%M')} ç«å“ç›‘æ§"
        
        logger.info(f"ğŸš€ å¼€å§‹ç«å“ç›‘æ§ä¼šè¯: {session_name}")
        
        # åˆ›å»ºçˆ¬å–ä¼šè¯
        session = CrawlSession(
            session_name=session_name,
            crawl_time=datetime.now(),
            status='processing'
        )
        db.session.add(session)
        db.session.commit()
        
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒçš„ç›‘æ§é…ç½®
            configs = MonitorConfig.query.filter_by(is_active=True).all()
            
            if not configs:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„ç›‘æ§é…ç½®")
                session.status = 'completed'
                session.ai_summary = "æ²¡æœ‰é…ç½®ç›‘æ§æº"
                db.session.commit()
                return {"success": False, "message": "æ²¡æœ‰é…ç½®ç›‘æ§æº"}
            
            all_posts = []
            total_posts = 0
            
            # éå†æ¯ä¸ªç›‘æ§é…ç½®
            for config in configs:
                logger.info(f"ğŸ“¡ çˆ¬å–é…ç½®: {config.name}")
                
                try:
                    # çˆ¬å–æ•°æ®
                    posts = self.crawler.crawl_by_config(config.to_dict())
                    
                    # å»é‡å’Œä¿å­˜
                    unique_posts = self._deduplicate_posts(posts, config.id, session.id)
                    
                    all_posts.extend(unique_posts)
                    total_posts += len(posts)
                    
                    # æ›´æ–°é…ç½®çš„æœ€åçˆ¬å–æ—¶é—´
                    config.last_crawl_time = datetime.now()
                    
                    logger.info(f"âœ… {config.name}: çˆ¬å– {len(posts)} æ¡ï¼Œå»é‡å {len(unique_posts)} æ¡")
                    
                except Exception as e:
                    logger.error(f"âŒ çˆ¬å–é…ç½®å¤±è´¥ {config.name}: {e}")
                    continue
            
            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            session.total_posts = total_posts
            session.processed_posts = len(all_posts)
            
            # AIåˆ†æ
            if all_posts:
                logger.info("ğŸ¤– å¼€å§‹AIåˆ†æ...")
                post_dicts = [post.to_dict() for post in all_posts]
                ai_summary = self.ai_service.analyze_posts(post_dicts)
                session.ai_summary = ai_summary
            else:
                session.ai_summary = "24å°æ—¶å†…æš‚æ— æ–°çš„ç«å“åŠ¨æ€"
            
            session.status = 'completed'
            db.session.commit()
            
            logger.info(f"ğŸ‰ ç«å“ç›‘æ§å®Œæˆ: å¤„ç† {len(all_posts)} æ¡æœ‰æ•ˆæ•°æ®")
            
            return {
                "success": True,
                "session_id": session.id,
                "total_posts": total_posts,
                "processed_posts": len(all_posts),
                "summary": session.ai_summary
            }
            
        except Exception as e:
            logger.error(f"âŒ ç«å“ç›‘æ§ä¼šè¯å¤±è´¥: {e}")
            session.status = 'failed'
            session.ai_summary = f"ç›‘æ§å¤±è´¥: {str(e)}"
            db.session.commit()
            
            return {
                "success": False,
                "message": str(e),
                "session_id": session.id
            }
        
        finally:
            # æ¸…ç†èµ„æº
            self.crawler.close()
    
    def _deduplicate_posts(self, posts: List, config_id: int, session_id: int) -> List[CompetitorPost]:
        """å»é‡å¹¶ä¿å­˜å¸–å­"""
        unique_posts = []
        
        for post_data in posts:
            # ç”Ÿæˆå†…å®¹hashç”¨äºå»é‡
            content_hash = self._generate_post_hash(
                post_data.title,
                post_data.post_url,
                post_data.author
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = CompetitorPost.query.filter_by(
                title=post_data.title,
                post_url=post_data.post_url
            ).first()
            
            if existing:
                existing.is_duplicate = True
                logger.debug(f"å‘ç°é‡å¤å¸–å­: {post_data.title[:50]}")
                continue
            
            # åˆ›å»ºæ–°è®°å½•
            new_post = CompetitorPost(
                session_id=session_id,
                monitor_config_id=config_id,
                title=post_data.title,
                content=post_data.content,
                author=post_data.author,
                post_url=post_data.post_url,
                post_time=post_data.post_time,
                likes_count=post_data.likes_count,
                comments_count=post_data.comments_count,
                platform=post_data.platform,
                is_processed=True
            )
            
            db.session.add(new_post)
            unique_posts.append(new_post)
        
        db.session.commit()
        return unique_posts
    
    def _generate_post_hash(self, title: str, url: str, author: str) -> str:
        """ç”Ÿæˆå¸–å­å†…å®¹çš„hashå€¼"""
        content = f"{title}|{url}|{author}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_recent_sessions(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„çˆ¬å–ä¼šè¯"""
        sessions = CrawlSession.query.order_by(
            CrawlSession.crawl_time.desc()
        ).limit(limit).all()
        
        return [session.to_dict() for session in sessions]
    
    def get_session_details(self, session_id: int) -> Optional[Dict[str, Any]]:
        """è·å–ä¼šè¯è¯¦æƒ…"""
        session = CrawlSession.query.get(session_id)
        if not session:
            return None
        
        session_data = session.to_dict()
        
        # è·å–å…³è”çš„å¸–å­
        posts = CompetitorPost.query.filter_by(session_id=session_id).all()
        session_data['posts'] = [post.to_dict() for post in posts]
        
        return session_data
    
    def add_monitor_config(self, config_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ ç›‘æ§é…ç½®"""
        try:
            config = MonitorConfig(
                name=config_data['name'],
                config_type=config_data['config_type'],
                account_url=config_data.get('account_url'),
                website_url=config_data.get('website_url'),
                keywords=config_data.get('keywords'),
                webpage_url=config_data.get('webpage_url'),
                content_hash=None,  # ç½‘é¡µæ›´æ–°æ¨¡å¼åˆå§‹åŒ–ä¸ºNoneï¼Œé¦–æ¬¡çˆ¬å–æ—¶è®¾ç½®
                last_content=None,  # ä¸Šæ¬¡å†…å®¹åˆå§‹åŒ–ä¸ºNone
                is_active=True
            )
            
            db.session.add(config)
            db.session.commit()
            
            logger.info(f"âœ… æ·»åŠ ç›‘æ§é…ç½®: {config.name}")
            
            return {
                "success": True,
                "config_id": config.id,
                "message": "ç›‘æ§é…ç½®æ·»åŠ æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç›‘æ§é…ç½®å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def update_monitor_config(self, config_id: int, config_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°ç›‘æ§é…ç½®"""
        try:
            config = MonitorConfig.query.get(config_id)
            if not config:
                return {"success": False, "message": "é…ç½®ä¸å­˜åœ¨"}
            
            config.name = config_data.get('name', config.name)
            config.config_type = config_data.get('config_type', config.config_type)
            config.account_url = config_data.get('account_url', config.account_url)
            config.website_url = config_data.get('website_url', config.website_url)
            config.keywords = config_data.get('keywords', config.keywords)
            config.is_active = config_data.get('is_active', config.is_active)
            
            db.session.commit()
            
            logger.info(f"âœ… æ›´æ–°ç›‘æ§é…ç½®: {config.name}")
            
            return {
                "success": True,
                "message": "ç›‘æ§é…ç½®æ›´æ–°æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç›‘æ§é…ç½®å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def delete_monitor_config(self, config_id: int) -> Dict[str, Any]:
        """åˆ é™¤ç›‘æ§é…ç½®"""
        try:
            config = MonitorConfig.query.get(config_id)
            if not config:
                return {"success": False, "message": "é…ç½®ä¸å­˜åœ¨"}
            
            db.session.delete(config)
            db.session.commit()
            
            logger.info(f"âœ… åˆ é™¤ç›‘æ§é…ç½®: {config.name}")
            
            return {
                "success": True,
                "message": "ç›‘æ§é…ç½®åˆ é™¤æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç›‘æ§é…ç½®å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def get_monitor_configs(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ç›‘æ§é…ç½®"""
        configs = MonitorConfig.query.order_by(MonitorConfig.created_at.desc()).all()
        return [config.to_dict() for config in configs]
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_configs = MonitorConfig.query.count()
        active_configs = MonitorConfig.query.filter_by(is_active=True).count()
        total_sessions = CrawlSession.query.count()
        total_posts = CompetitorPost.query.count()
        
        # æœ€è¿‘24å°æ—¶çš„æ•°æ®
        yesterday = datetime.now() - timedelta(days=1)
        recent_sessions = CrawlSession.query.filter(
            CrawlSession.crawl_time >= yesterday
        ).count()
        recent_posts = CompetitorPost.query.filter(
            CompetitorPost.created_at >= yesterday
        ).count()
        
        return {
            "total_configs": total_configs,
            "active_configs": active_configs,
            "total_sessions": total_sessions,
            "total_posts": total_posts,
            "recent_sessions": recent_sessions,
            "recent_posts": recent_posts,
            "ai_status": self.ai_service.get_summary_status()
        }
    
    def execute_scheduled_crawl(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®šæ—¶çˆ¬å–ä»»åŠ¡ï¼ˆæ¯æ—¥10ç‚¹ï¼‰ï¼Œæœ‰å†…å®¹æ—¶æ¨é€é£ä¹¦"""
        logger.info("ğŸ•˜ æ‰§è¡Œå®šæ—¶ç«å“ç›‘æ§...")
        
        try:
            # æ‰§è¡Œçˆ¬å–ä¼šè¯
            result = self.execute_crawl_session("æ¯æ—¥å®šæ—¶ç›‘æ§")
            
            # å¦‚æœæœ‰æ–°å†…å®¹ä¸”çˆ¬å–æˆåŠŸï¼Œå‘é€é£ä¹¦æ¨é€
            if result.get("success") and result.get("processed_posts", 0) > 0:
                logger.info("ğŸ“± å‡†å¤‡å‘é€é£ä¹¦æ¨é€...")
                
                # è·å–æœ¬æ¬¡ä¼šè¯çš„å¸–å­æ•°æ®
                session_id = result.get("session_id")
                if session_id:
                    posts = CompetitorPost.query.filter_by(session_id=session_id).all()
                    post_dicts = [post.to_dict() for post in posts]
                    
                    # å‘é€é£ä¹¦æ¨é€
                    feishu_success = self.feishu_service.send_daily_summary(
                        post_dicts, 
                        "æ¯æ—¥å®šæ—¶ç›‘æ§"
                    )
                    
                    if feishu_success:
                        logger.info("âœ… é£ä¹¦æ¨é€å‘é€æˆåŠŸ")
                        result["feishu_sent"] = True
                    else:
                        logger.warning("âš ï¸ é£ä¹¦æ¨é€å‘é€å¤±è´¥")
                        result["feishu_sent"] = False
                else:
                    logger.warning("âš ï¸ æ— æ³•è·å–ä¼šè¯IDï¼Œè·³è¿‡é£ä¹¦æ¨é€")
                    result["feishu_sent"] = False
            else:
                logger.info("ğŸ“± æ²¡æœ‰æ–°å†…å®¹ï¼Œè·³è¿‡é£ä¹¦æ¨é€")
                result["feishu_sent"] = False
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å®šæ—¶ç›‘æ§å¤±è´¥: {e}")
            return {
                "success": False,
                "message": str(e),
                "feishu_sent": False
            }
    
    def test_feishu_webhook(self) -> bool:
        """æµ‹è¯•é£ä¹¦webhookè¿æ¥"""
        return self.feishu_service.test_webhook()
    
    def update_feishu_webhook(self, webhook_url: str):
        """æ›´æ–°é£ä¹¦webhookåœ°å€"""
        self.feishu_service.update_webhook_url(webhook_url)
    
    def delete_crawl_session(self, session_id: int) -> Dict[str, Any]:
        """åˆ é™¤çˆ¬å–ä¼šè¯å’Œç›¸å…³çš„æ‰€æœ‰å¸–å­è®°å½•"""
        try:
            session = CrawlSession.query.get(session_id)
            if not session:
                return {"success": False, "message": "ä¼šè¯ä¸å­˜åœ¨"}
            
            session_name = session.session_name
            
            # åˆ é™¤å…³è”çš„å¸–å­è®°å½•
            posts_deleted = CompetitorPost.query.filter_by(session_id=session_id).delete()
            
            # åˆ é™¤ä¼šè¯è®°å½•
            db.session.delete(session)
            db.session.commit()
            
            logger.info(f"âœ… åˆ é™¤çˆ¬å–ä¼šè¯: {session_name}ï¼Œåˆ é™¤å¸–å­ {posts_deleted} æ¡")
            
            return {
                "success": True,
                "message": f"æˆåŠŸåˆ é™¤ä¼šè¯ '{session_name}' å’Œç›¸å…³çš„ {posts_deleted} æ¡å¸–å­è®°å½•",
                "deleted_posts": posts_deleted
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤çˆ¬å–ä¼šè¯å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def delete_posts_by_url(self, post_urls: List[str]) -> Dict[str, Any]:
        """æ ¹æ®URLåˆ é™¤ç‰¹å®šå¸–å­è®°å½•"""
        try:
            deleted_count = 0
            
            for url in post_urls:
                posts = CompetitorPost.query.filter_by(post_url=url).all()
                for post in posts:
                    db.session.delete(post)
                    deleted_count += 1
            
            db.session.commit()
            
            logger.info(f"âœ… åˆ é™¤å¸–å­è®°å½•: {deleted_count} æ¡")
            
            return {
                "success": True,
                "message": f"æˆåŠŸåˆ é™¤ {deleted_count} æ¡å¸–å­è®°å½•",
                "deleted_count": deleted_count
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å¸–å­è®°å½•å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def delete_posts_by_session_and_config(self, session_id: int, config_id: int) -> Dict[str, Any]:
        """åˆ é™¤ç‰¹å®šä¼šè¯å’Œé…ç½®çš„å¸–å­è®°å½•"""
        try:
            deleted_count = CompetitorPost.query.filter_by(
                session_id=session_id,
                monitor_config_id=config_id
            ).delete()
            
            db.session.commit()
            
            logger.info(f"âœ… åˆ é™¤ç‰¹å®šé…ç½®çš„å¸–å­è®°å½•: {deleted_count} æ¡")
            
            return {
                "success": True,
                "message": f"æˆåŠŸåˆ é™¤ {deleted_count} æ¡å¸–å­è®°å½•",
                "deleted_count": deleted_count
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å¸–å­è®°å½•å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def clear_old_records(self, days_before: int = 30) -> Dict[str, Any]:
        """æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„è€è®°å½•"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_before)
            
            # åˆ é™¤è€çš„å¸–å­è®°å½•
            posts_deleted = CompetitorPost.query.filter(
                CompetitorPost.created_at < cutoff_date
            ).delete()
            
            # åˆ é™¤è€çš„ä¼šè¯è®°å½•
            sessions_deleted = CrawlSession.query.filter(
                CrawlSession.crawl_time < cutoff_date
            ).delete()
            
            db.session.commit()
            
            logger.info(f"âœ… æ¸…ç†è€è®°å½•: åˆ é™¤ {sessions_deleted} ä¸ªä¼šè¯ï¼Œ{posts_deleted} æ¡å¸–å­")
            
            return {
                "success": True,
                "message": f"æˆåŠŸæ¸…ç† {days_before} å¤©å‰çš„è®°å½•ï¼š{sessions_deleted} ä¸ªä¼šè¯ï¼Œ{posts_deleted} æ¡å¸–å­",
                "deleted_sessions": sessions_deleted,
                "deleted_posts": posts_deleted
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è€è®°å½•å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def get_session_posts(self, session_id: int) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šä¼šè¯çš„æ‰€æœ‰å¸–å­"""
        posts = CompetitorPost.query.filter_by(session_id=session_id).order_by(
            CompetitorPost.created_at.desc()
        ).all()
        
        return [post.to_dict() for post in posts] 