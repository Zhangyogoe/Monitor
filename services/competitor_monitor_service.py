#!/usr/bin/env python3
"""
ç«žå“ç›‘æŽ§æ ¸å¿ƒæœåŠ¡
æ•´åˆçˆ¬è™«ã€AIåˆ†æžã€æ•°æ®å­˜å‚¨ç­‰åŠŸèƒ½
"""

from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from models.competitor_models import db, MonitorConfig, CrawlSession, CompetitorPost
from crawlers.competitor_crawler import CompetitorCrawler
from services.competitor_ai_service import CompetitorAIService
from loguru import logger
import hashlib

class CompetitorMonitorService:
    """ç«žå“ç›‘æŽ§æ ¸å¿ƒæœåŠ¡"""
    
    def __init__(self):
        self.crawler = CompetitorCrawler()
        self.ai_service = CompetitorAIService()
    
    def execute_crawl_session(self, session_name: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬å–ä¼šè¯"""
        if not session_name:
            session_name = f"{datetime.now().strftime('%Y-%m-%d %H:%M')} ç«žå“ç›‘æŽ§"
        
        logger.info(f"ðŸš€ å¼€å§‹ç«žå“ç›‘æŽ§ä¼šè¯: {session_name}")
        
        # åˆ›å»ºçˆ¬å–ä¼šè¯
        session = CrawlSession(
            session_name=session_name,
            crawl_time=datetime.now(),
            status='processing'
        )
        db.session.add(session)
        db.session.commit()
        
        try:
            # èŽ·å–æ‰€æœ‰æ´»è·ƒçš„ç›‘æŽ§é…ç½®
            configs = MonitorConfig.query.filter_by(is_active=True).all()
            
            if not configs:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„ç›‘æŽ§é…ç½®")
                session.status = 'completed'
                session.ai_summary = "æ²¡æœ‰é…ç½®ç›‘æŽ§æº"
                db.session.commit()
                return {"success": False, "message": "æ²¡æœ‰é…ç½®ç›‘æŽ§æº"}
            
            all_posts = []
            total_posts = 0
            
            # éåŽ†æ¯ä¸ªç›‘æŽ§é…ç½®
            for config in configs:
                logger.info(f"ðŸ“¡ çˆ¬å–é…ç½®: {config.name}")
                
                try:
                    # çˆ¬å–æ•°æ®
                    posts = self.crawler.crawl_by_config(config.to_dict())
                    
                    # åŽ»é‡å’Œä¿å­˜
                    unique_posts = self._deduplicate_posts(posts, config.id, session.id)
                    
                    all_posts.extend(unique_posts)
                    total_posts += len(posts)
                    
                    # æ›´æ–°é…ç½®çš„æœ€åŽçˆ¬å–æ—¶é—´
                    config.last_crawl_time = datetime.now()
                    
                    logger.info(f"âœ… {config.name}: çˆ¬å– {len(posts)} æ¡ï¼ŒåŽ»é‡åŽ {len(unique_posts)} æ¡")
                    
                except Exception as e:
                    logger.error(f"âŒ çˆ¬å–é…ç½®å¤±è´¥ {config.name}: {e}")
                    continue
            
            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            session.total_posts = total_posts
            session.processed_posts = len(all_posts)
            
            # AIåˆ†æž
            if all_posts:
                logger.info("ðŸ¤– å¼€å§‹AIåˆ†æž...")
                post_dicts = [post.to_dict() for post in all_posts]
                ai_summary = self.ai_service.analyze_posts(post_dicts)
                session.ai_summary = ai_summary
            else:
                session.ai_summary = "24å°æ—¶å†…æš‚æ— æ–°çš„ç«žå“åŠ¨æ€"
            
            session.status = 'completed'
            db.session.commit()
            
            logger.info(f"ðŸŽ‰ ç«žå“ç›‘æŽ§å®Œæˆ: å¤„ç† {len(all_posts)} æ¡æœ‰æ•ˆæ•°æ®")
            
            return {
                "success": True,
                "session_id": session.id,
                "total_posts": total_posts,
                "processed_posts": len(all_posts),
                "summary": session.ai_summary
            }
            
        except Exception as e:
            logger.error(f"âŒ ç«žå“ç›‘æŽ§ä¼šè¯å¤±è´¥: {e}")
            session.status = 'failed'
            session.ai_summary = f"ç›‘æŽ§å¤±è´¥: {str(e)}"
            db.session.commit()
            
            return {
                "success": False,
                "message": str(e),
                "session_id": session.id
            }
        
        finally:
            # æ¸…ç†èµ„æº
            self.crawler.close()
    
    def _deduplicate_posts(self, posts: List, config_id: int, session_id: int) -> List[CompetitorPost]:
        """åŽ»é‡å¹¶ä¿å­˜å¸–å­"""
        unique_posts = []
        
        for post_data in posts:
            # ç”Ÿæˆå†…å®¹hashç”¨äºŽåŽ»é‡
            content_hash = self._generate_post_hash(
                post_data.title,
                post_data.post_url,
                post_data.author
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = CompetitorPost.query.filter_by(
                title=post_data.title,
                post_url=post_data.post_url
            ).first()
            
            if existing:
                existing.is_duplicate = True
                logger.debug(f"å‘çŽ°é‡å¤å¸–å­: {post_data.title[:50]}")
                continue
            
            # åˆ›å»ºæ–°è®°å½•
            new_post = CompetitorPost(
                session_id=session_id,
                monitor_config_id=config_id,
                title=post_data.title,
                content=post_data.content,
                author=post_data.author,
                post_url=post_data.post_url,
                post_time=post_data.post_time,
                likes_count=post_data.likes_count,
                comments_count=post_data.comments_count,
                platform=post_data.platform,
                is_processed=True
            )
            
            db.session.add(new_post)
            unique_posts.append(new_post)
        
        db.session.commit()
        return unique_posts
    
    def _generate_post_hash(self, title: str, url: str, author: str) -> str:
        """ç”Ÿæˆå¸–å­å†…å®¹çš„hashå€¼"""
        content = f"{title}|{url}|{author}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_recent_sessions(self, limit: int = 10) -> List[Dict[str, Any]]:
        """èŽ·å–æœ€è¿‘çš„çˆ¬å–ä¼šè¯"""
        sessions = CrawlSession.query.order_by(
            CrawlSession.crawl_time.desc()
        ).limit(limit).all()
        
        return [session.to_dict() for session in sessions]
    
    def get_session_details(self, session_id: int) -> Optional[Dict[str, Any]]:
        """èŽ·å–ä¼šè¯è¯¦æƒ…"""
        session = CrawlSession.query.get(session_id)
        if not session:
            return None
        
        session_data = session.to_dict()
        
        # èŽ·å–å…³è”çš„å¸–å­
        posts = CompetitorPost.query.filter_by(session_id=session_id).all()
        session_data['posts'] = [post.to_dict() for post in posts]
        
        return session_data
    
    def add_monitor_config(self, config_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ ç›‘æŽ§é…ç½®"""
        try:
            config = MonitorConfig(
                name=config_data['name'],
                config_type=config_data['config_type'],
                account_url=config_data.get('account_url'),
                website_url=config_data.get('website_url'),
                keywords=config_data.get('keywords'),
                webpage_url=config_data.get('webpage_url'),
                content_hash=None,  # ç½‘é¡µæ›´æ–°æ¨¡å¼åˆå§‹åŒ–ä¸ºNoneï¼Œé¦–æ¬¡çˆ¬å–æ—¶è®¾ç½®
                last_content=None,  # ä¸Šæ¬¡å†…å®¹åˆå§‹åŒ–ä¸ºNone
                is_active=True
            )
            
            db.session.add(config)
            db.session.commit()
            
            logger.info(f"âœ… æ·»åŠ ç›‘æŽ§é…ç½®: {config.name}")
            
            return {
                "success": True,
                "config_id": config.id,
                "message": "ç›‘æŽ§é…ç½®æ·»åŠ æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç›‘æŽ§é…ç½®å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def update_monitor_config(self, config_id: int, config_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°ç›‘æŽ§é…ç½®"""
        try:
            config = MonitorConfig.query.get(config_id)
            if not config:
                return {"success": False, "message": "é…ç½®ä¸å­˜åœ¨"}
            
            config.name = config_data.get('name', config.name)
            config.config_type = config_data.get('config_type', config.config_type)
            config.account_url = config_data.get('account_url', config.account_url)
            config.website_url = config_data.get('website_url', config.website_url)
            config.keywords = config_data.get('keywords', config.keywords)
            config.is_active = config_data.get('is_active', config.is_active)
            
            db.session.commit()
            
            logger.info(f"âœ… æ›´æ–°ç›‘æŽ§é…ç½®: {config.name}")
            
            return {
                "success": True,
                "message": "ç›‘æŽ§é…ç½®æ›´æ–°æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç›‘æŽ§é…ç½®å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def delete_monitor_config(self, config_id: int) -> Dict[str, Any]:
        """åˆ é™¤ç›‘æŽ§é…ç½®"""
        try:
            config = MonitorConfig.query.get(config_id)
            if not config:
                return {"success": False, "message": "é…ç½®ä¸å­˜åœ¨"}
            
            db.session.delete(config)
            db.session.commit()
            
            logger.info(f"âœ… åˆ é™¤ç›‘æŽ§é…ç½®: {config.name}")
            
            return {
                "success": True,
                "message": "ç›‘æŽ§é…ç½®åˆ é™¤æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç›‘æŽ§é…ç½®å¤±è´¥: {e}")
            db.session.rollback()
            
            return {
                "success": False,
                "message": str(e)
            }
    
    def get_monitor_configs(self) -> List[Dict[str, Any]]:
        """èŽ·å–æ‰€æœ‰ç›‘æŽ§é…ç½®"""
        configs = MonitorConfig.query.order_by(MonitorConfig.created_at.desc()).all()
        return [config.to_dict() for config in configs]
    
    def get_statistics(self) -> Dict[str, Any]:
        """èŽ·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_configs = MonitorConfig.query.count()
        active_configs = MonitorConfig.query.filter_by(is_active=True).count()
        total_sessions = CrawlSession.query.count()
        total_posts = CompetitorPost.query.count()
        
        # æœ€è¿‘24å°æ—¶çš„æ•°æ®
        yesterday = datetime.now() - timedelta(days=1)
        recent_sessions = CrawlSession.query.filter(
            CrawlSession.crawl_time >= yesterday
        ).count()
        recent_posts = CompetitorPost.query.filter(
            CompetitorPost.created_at >= yesterday
        ).count()
        
        return {
            "total_configs": total_configs,
            "active_configs": active_configs,
            "total_sessions": total_sessions,
            "total_posts": total_posts,
            "recent_sessions": recent_sessions,
            "recent_posts": recent_posts,
            "ai_status": self.ai_service.get_summary_status()
        } 